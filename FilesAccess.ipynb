{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading csv file from Amazon S3 into iguazio File system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/v3io/bigdata/examples/': File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  975k  100  975k    0     0  7913k      0 --:--:-- --:--:-- --:--:-- 7931k\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir /v3io/bigdata/examples/\n",
    "curl -L \"deutsche-boerse-xetra-pds.s3.amazonaws.com/2018-03-26/2018-03-26_BINS_XETR07.csv\" > /v3io/bigdata/examples/stocks_example.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and write the file using Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Iguazio Integration demo\").getOrCreate()\n",
    "\n",
    "# Read the sample stocks.csv file into a Spark DataFrame, and let Spark infer the schema of the CSV file\n",
    "myDF = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"v3io://bigdata/examples/stocks_example.csv\")\n",
    "\n",
    "# Write the DataFrame data to a stocks_nosql table under \"bigdata\" container and define \"ISIN\" column as a key\n",
    "myDF.write.format(\"io.iguaz.v3io.spark.sql.kv\").mode(\"append\").option(\"key\", \"ISIN\").save(\"v3io://bigdata/examples/stocks_tab/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read iguazio table and writing it back as a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF2 = spark.read.format(\"io.iguaz.v3io.spark.sql.kv\").load(\"v3io://bigdata/examples/stocks_tab\").where(\"TradedVolume>20000\")\n",
    "\n",
    "# myDF2.write.csv('v3io://bigdata/examples/stocks_high_volume.csv')\n",
    "myDF2.coalesce(1).write.csv('v3io://bigdata/examples/stocks_high_volume.csv')\n",
    "\n",
    "# note that using coalesce(1) is for storing the output as a single file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted v3io://bigdata/examples/stocks_example.csv\n",
      "Deleted v3io://bigdata/examples/stocks_high_volume.csv\n",
      "Deleted v3io://bigdata/examples/stocks_tab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18/11/13 09:41:45 INFO slf_4j.Slf4jLogger: Slf4jLogger started\n",
      "18/11/13 09:41:46 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "hadoop fs -rm -r v3io://bigdata/examples/stock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
